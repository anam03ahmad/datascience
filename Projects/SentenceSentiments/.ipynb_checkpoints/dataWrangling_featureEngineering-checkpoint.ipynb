{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Wrangling and Feature Engineering:\n",
    "    \n",
    "In this step, I am going to break out all the words into a list (tokenizing) and reduce them down to their root (lemmitizing). From there I can determine what language the reviews are, label the words, etc.  \n",
    "\n",
    "1. First, I need to clean the data  \n",
    "    i. determine the language  \n",
    "    ii. removing any nonalphanumeric character  \n",
    "    iii. remove stop words, as they are not useful for predictions, but cause a lot of noise in the data  \n",
    "2. Tokenize: create a list of all the words (referred to as tokens)\n",
    "3. Lemmatization: reducing the words to their root\n",
    "4. N-gram labels\n",
    "\n",
    "I will use this to extract some predictive features that the model will be able to use to determine the sentiment of a given review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:/Users/Anam/Documents/DataScience/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from langdetect import detect\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "pd.options.mode.chained_assignment = None\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews records: 3000\n"
     ]
    }
   ],
   "source": [
    "reviews_file = PATH +\"datascience/Projects/SentenceSentiments/Data/reviews.csv\"\n",
    "reviews_df = pd.read_csv(reviews_file, sep='\\t', header = 0, quoting=csv.QUOTE_NONE)\n",
    "print(\"Reviews records: \" + str(len(reviews_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(text_series):\n",
    "    languages = []\n",
    "    for s in text_series:\n",
    "        languages.append(detect(s))\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = re.sub('[\\']+', '', s)\n",
    "    s = re.sub('[\\W_]+', ' ', s)\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(shot, NN), (in, IN), (the, DT), (southern, J...\n",
       "1    [(my, PRP$), (order, NN), (was, VBD), (not, RB...\n",
       "2    [(this, DT), (place, NN), (should, MD), (hones...\n",
       "3    [(it, PRP), (makes, VBZ), (very, RB), (strange...\n",
       "4                            [(nice, JJ), (sound, NN)]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['tokens'] = reviews_df['review'].transform(lambda x: clean_text(x))\n",
    "reviews_df['tokens'] = reviews_df['tokens'].transform(lambda x: pos_tag(word_tokenize(x)))\n",
    "reviews_df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wordnet_tag(tokens):\n",
    "\n",
    "    for word, tag in tokens:\n",
    "        if tag.startswith('J'):\n",
    "            tag = wordnet.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            tag = wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            tag = wordnet.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            tag = wordnet.VERB\n",
    "        else:\n",
    "            tag = ''\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(shot, NN), (in, IN), (the, DT), (southern, J...\n",
       "1    [(my, PRP$), (order, NN), (was, VBD), (not, RB...\n",
       "2    [(this, DT), (place, NN), (should, MD), (hones...\n",
       "3    [(it, PRP), (makes, VBZ), (very, RB), (strange...\n",
       "4                            [(nice, JJ), (sound, NN)]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['tokens'] = reviews_df['tokens'].transform(lambda x: convert_to_wordnet_tag(x))\n",
    "reviews_df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "reviews_df['tokens'] = reviews_df['tokens'].transform(lambda x: [w for w in x if not w in stop_words])\n",
    "reviews_df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemma = []\n",
    "    for word, tag in tokens:\n",
    "        meaningful_word_tag = tag[0].lower()\n",
    "        meaningful_word_tag = meaningful_word_tag if meaningful_word_tag in ['a', 's', 'r', 'n', 'v'] else None\n",
    "        if meaningful_word_tag:\n",
    "            lemma.append(lemmatizer.lemmatize(word, meaningful_word_tag))\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['tokens'] = reviews_df['tokens'].transform(lambda x: lemmatize(x))\n",
    "reviews_df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
